% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/RcppExports.R
\name{lda_cgs_em_perplexity}
\alias{lda_cgs_em_perplexity}
\title{LDA: Gibbs-EM with Perplexity Computation}
\usage{
lda_cgs_em_perplexity(num_topics, vocab_size, docs_tf, alpha_h, eta_h,
  em_max_iter, gibbs_max_iter, burn_in, spacing, save_theta, save_beta, save_lp,
  verbose, test_set_share)
}
\arguments{
\item{num_topics}{Number of topics in the corpus}

\item{vocab_size}{Vocabulary size}

\item{docs_tf}{A list of corpus documents read from the Blei corpus using
\code{\link{read_docs}} (term indices starts with 0)}

\item{alpha_h}{Hyperparameter for \eqn{\theta} sampling}

\item{eta_h}{Smoothing parameter for the \eqn{\beta} matrix}

\item{em_max_iter}{Maximum number of EM iterations to be performed}

\item{gibbs_max_iter}{Maximum number of Gibbs iterations to be performed}

\item{burn_in}{Burn-in-period for the Gibbs sampler}

\item{spacing}{Spacing between the stored samples (to reduce correlation)}

\item{save_theta}{if 0 the function does not save \eqn{\theta} samples}

\item{save_beta}{if 0 the function does not save \eqn{\beta} samples}

\item{save_lp}{if 0 the function does not save computed log posterior for
iterations}

\item{verbose}{from {0, 1, 2}}

\item{test_set_share}{proportion of the test words in each document. Must be
between 0. and 1.}
}
\value{
The Markov chain output as a list of
  \item{corpus_topic_counts}{corpus-level topic counts from last iteration
  of the Markov chain}
  \item{theta_counts}{document-level topic counts from last iteration
  of the Markov chain}
  \item{beta_counts}{topic word counts from last iteration of the Markov chain}
  \item{theta_samples}{\eqn{\theta} samples after the burn in period, if
  \code{save_theta} is set}
  \item{beta_samples}{\eqn{\beta} samples after the burn in period, if
  \code{save_beta} is set}
  \item{log_posterior}{the log posterior (upto a constant multiplier) of
  the hidden variable \eqn{\psi = (\beta, \theta, z)} in the LDA model,
  if \code{save_lp} is set}
  \item{perplexity}{perplexity of the held-out words' set}
}
\description{
This implements the Gibbs-EM algorithm for LDA that is mentioned in the  
paper Topic Modeling: Beyond Bag-of-Words. Wallach (2006).
}
\details{
It uses the LDA collapsed Gibbs sampler---a Markov chain on \eqn{z} for the
E-step, and Minka (2003) fixed point iterations to optimize \eqn{h = (\eta,
\alpha)} in the M-step. To compute perplexity, it first partitions each
document in the corpus into two sets of words: (a) a test set (held-out set)
and (b) a training set, given a user defined \code{test_set_share}. Then, it
runs the Markov chain based on the training set and computes perplexity for
the held-out set.
}
\seealso{
Other MCMC: \code{\link{lda_acgs_st}},
  \code{\link{lda_cgs_em}},
  \code{\link{lda_cgs_perplexity}},
  \code{\link{lda_fgs_BF_perplexity}},
  \code{\link{lda_fgs_perplexity}},
  \code{\link{lda_fgs_ppc}},
  \code{\link{lda_fgs_st_perplexity}}
}
