% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/RcppExports.R
\name{lda_acgs_st}
\alias{lda_acgs_st}
\title{LDA: Serial Tempering with Perplexity Computation}
\usage{
lda_acgs_st(num_topics, vocab_size, docs_tf, h_grid, st_grid, st_grid_nbrs,
  init_st_grid_index, zetas, tuning_iter, max_iter_tuning, max_iter_final,
  burn_in, spacing, test_set_share, save_beta, save_theta, save_lp,
  save_hat_ratios, save_tilde_ratios, verbose)
}
\arguments{
\item{num_topics}{Number of topics in the corpus}

\item{vocab_size}{Vocabulary size}

\item{docs_tf}{A list of corpus documents read from the Blei corpus using 
\code{\link{read_docs}} (term indices starts with 0)}

\item{h_grid}{A 2-dimensional grid of hyperparameters \eqn{h = (\eta, 
\alpha)}. It is a 2 x G matrix, where G is the number of grid points and 
the first row is for \eqn{\alpha} values and the second row is for 
\eqn{\eta} values}

\item{st_grid}{A 2-dimensional grid of hyperparameters \eqn{h = (\eta, 
\alpha)}. It is a 2 x G matrix, where G is the number of grid points and 
the first row is for \eqn{\alpha} values and the second row is for 
\eqn{\eta} values. This a subgrid on h_grid_ that is used for Serial 
Tempering}

\item{st_grid_nbrs}{The neighbor indices, from [0, G-1], of each helper grid
point}

\item{init_st_grid_index}{Index of the helper h grid, from [1, G], of the 
initial hyperparameter \eqn{h = (\eta, \alpha)}}

\item{zetas}{Initial guess for normalization constants}

\item{tuning_iter}{Number of tuning iterations}

\item{max_iter_tuning}{Maximum number of Gibbs iterations to be performed
for the tuning iterations}

\item{max_iter_final}{Maximum number of Gibbs iterations to be performed for
the final run}

\item{burn_in}{Burn-in-period for the Gibbs sampler}

\item{spacing}{Spacing between the stored samples (to reduce correlation)}

\item{test_set_share}{Proportion of the test words in each document. Must be
between 0. and 1.}

\item{save_beta}{If 0 the function does not save \eqn{\beta} samples}

\item{save_theta}{If 0 the function does not save \eqn{\theta} samples}

\item{save_lp}{if 0 The function does not save computed log posterior for 
iterations}

\item{save_hat_ratios}{If 0 the function does not save hat ratios for 
iterations}

\item{save_tilde_ratios}{If 0 the function does not save tilde ratios for 
iterations}

\item{verbose}{Values from {0, 1, 2}}
}
\value{
A list of
  \item{corpus_topic_counts}{corpus-level topic counts from last iteration
  of the Markov chain}
  \item{theta_counts}{document-level topic counts from last iteration
  of the Markov chain}
  \item{beta_counts}{topic word counts from last iteration of the Markov chain}
  \item{theta_samples}{\eqn{\theta} samples after the burn in period, if
  \code{save_theta} is set}
  \item{beta_samples}{\eqn{\beta} samples after the burn in period, if
  \code{save_beta} is set}
  \item{log_posterior}{the log posterior (upto a constant multiplier) of
  the hidden variable \eqn{\psi = (\beta, \theta, z)} in the LDA model,
  if \code{save_lp} is set}
  \item{perplexity}{perplexity of the held-out words' set}
}
\description{
Implements the LDA serial tempering algorithm. Sampling \code{z_{di}}'s 
is adapted from the idea of collapsed Gibbs sampling chain (Griffiths and 
Steyvers, 2004). To compute perplexity, it first partitions each document in 
the corpus into two sets of words: 
  (a) a test set (held-out set) and 
  (b) a training set, given a user defined \eqn{test_set_share}. 
Then, it runs the Markov chain based on the training set and computes 
perplexity for the held-out set.
}
\note{
Modifed on:
 
 October 01, 2016 - Created date, adapated from lda_fgs_st.cpp
}
\seealso{
Other MCMC: \code{\link{lda_cgs_em_perplexity}},
  \code{\link{lda_cgs_em}},
  \code{\link{lda_cgs_perplexity}},
  \code{\link{lda_fgs_BF_perplexity}},
  \code{\link{lda_fgs_perplexity}},
  \code{\link{lda_fgs_ppc}},
  \code{\link{lda_fgs_st_perplexity}}
}
